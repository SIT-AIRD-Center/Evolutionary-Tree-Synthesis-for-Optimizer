{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.node import Node\n",
    "from local.constnode import ConstNode\n",
    "from local.varnode import VarNode\n",
    "from local.funcnode import FuncNode\n",
    "from local.func import *\n",
    "from local.treeoptimizer import TreeOptimizer\n",
    "from local.crossover import *\n",
    "from local.Optimizer import *\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import graphviz\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, losses, metrics, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder():\n",
    "    date = datetime.datetime.today()\n",
    "    folder_path = str(date.year)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "    folder_path += \"/\"+str(date.month) +\"_\"+str(date.day)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "    folder_path += \"/\"+str(date.hour)+\"_\"+str(date.minute)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "        \n",
    "    return folder_path\n",
    "\n",
    "\n",
    "def Tounament_select(score_list, generation, Selection_size):\n",
    "    candidate = random.sample(range(len(score_list)), Selection_size)\n",
    "    candidate_score = [score_list[i] for i in candidate]\n",
    "    index = candidate_score.index(max(candidate_score))\n",
    "    return copy.deepcopy(generation[candidate[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Population = 50\n",
    "Max_generation = 500\n",
    "\n",
    "Mutation_rate = 0.2\n",
    "Crossover_rate = 0.8\n",
    "\n",
    "Selection_size = 2\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 10\n",
    "\n",
    "loss_obj = metrics.Mean()\n",
    "accuracy_obj = metrics.SparseCategoricalAccuracy()\n",
    "val_loss_obj = metrics.SparseCategoricalCrossentropy()\n",
    "val_accuracy_obj = metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "generation = []\n",
    "generation += [Momentum(learning_rate=0.01) for _ in range(Population // 2)]\n",
    "generation += [RMSProp() for _ in range(Population // 2)]\n",
    "\n",
    "best_optim = SGD()\n",
    "best_score = 0.0\n",
    "best_score_list = []\n",
    "past_progress = 0\n",
    "folder_path = create_folder()\n",
    "if not os.path.exists(folder_path+\"/best_optim/\"):\n",
    "    os.mkdir(folder_path+\"/best_optim/\")\n",
    "score_progress = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "    x_train, x_test = x_train[..., tf.newaxis] / 255.0, x_test[..., tf.newaxis] / 255.0\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def build_model():\n",
    "    input = layers.Input(shape = (32, 32, 3))\n",
    "    x = layers.Conv2D(32, 3, 2)(input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units = 128, activation = \"relu\")(x)\n",
    "    output = layers.Dense(units = 10, activation = \"softmax\")(x)\n",
    "\n",
    "    return models.Model(input, output)\n",
    "\n",
    "train_dataset, test_dataset = load_data()\n",
    "\n",
    "model = build_model()\n",
    "model.compile()\n",
    "if not os.path.exists(\"models/\"):\n",
    "    os.mkdir(\"models/\")\n",
    "if not os.path.exists(\"models/model_cifar.keras\"):\n",
    "    model.save_weights(\"models/model_cifar.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score = 0.6003\n",
    "# folder_path = '2024/1_2/10_4'\n",
    "# read_folder_path = folder_path +f\"/generation{past_progress}\"\n",
    "# pickle_list = glob.glob(read_folder_path +\"/*.pickle\")\n",
    "# generation = []\n",
    "# for path in pickle_list:\n",
    "#     pickle_file = open(path, \"rb\")\n",
    "#     genetic = pickle.load(pickle_file)\n",
    "#     generation.append(genetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for progress in range(past_progress, Max_generation):\n",
    "    fitness_list = []\n",
    "    save_path = folder_path + f\"/generation{progress}/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    for index in tqdm(range(len(generation))):\n",
    "\n",
    "        acc_list = []\n",
    "        optimizer = generation[index]\n",
    "        optimizer.model_params = list()\n",
    "        optimizer.grads_params = list()\n",
    "        model.load_weights(\"models/model_cifar.keras\")\n",
    "        penalty = 1e-5 * len(optimizer.make_struct_dict().keys())\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(X, Y):\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(X)\n",
    "                loss = losses.SparseCategoricalCrossentropy()(Y, pred)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            loss_obj(loss)\n",
    "            accuracy_obj(Y, pred)\n",
    "        \n",
    "        @tf.function\n",
    "        def test_step(X, Y):\n",
    "            pred = model(X)\n",
    "            \n",
    "            val_loss_obj(Y, pred)\n",
    "            val_accuracy_obj(Y, pred)\n",
    "\n",
    "        for i in range(EPOCHS):\n",
    "            \n",
    "            loss_obj.reset_state()\n",
    "            accuracy_obj.reset_state()\n",
    "            val_loss_obj.reset_state()\n",
    "            val_accuracy_obj.reset_state()\n",
    "            gradients = 0\n",
    "            for X, Y in train_dataset:\n",
    "                train_step(X, Y)\n",
    "            \n",
    "            for X, Y in test_dataset:\n",
    "                test_step(X, Y)\n",
    "        \n",
    "            acc_list.append(val_accuracy_obj.result().numpy())\n",
    "        fitness_list.append(max(acc_list))\n",
    "        # 画像保存に時間がかかったのでコメントアウトしてます\n",
    "        # optimizer.plot_struct(f\"{save_path}/optimizer_{index}_{max(acc_list)}\")\n",
    "        pickle_file = open(f\"{save_path}/optimizer_{index}_{max(acc_list)}.pickle\", mode= \"wb\")\n",
    "        pickle.dump(optimizer, pickle_file)\n",
    "        pickle_file.close()\n",
    "\n",
    "\n",
    "    for index in range(len(fitness_list)):\n",
    "        if fitness_list[index] > best_score:\n",
    "            best_score = fitness_list[index]\n",
    "            best_optim = copy.deepcopy(generation[index])\n",
    "\n",
    "            best_optim.plot_struct(f\"{folder_path}/best_optim/generation_{progress}_{index}_{best_score}\")\n",
    "            pickle_file = open(f\"{folder_path}/best_optim/generation_{progress}_{index}_{best_score}.pickle\", mode= \"wb\")\n",
    "            pickle.dump(best_optim, pickle_file)\n",
    "            pickle_file.close()\n",
    "    \n",
    "    best_score_list.append(best_score)\n",
    "    print(best_score)\n",
    "    json_file = open(f\"{folder_path}/best_optim/score{progress}.json\", \"w\")\n",
    "    json.dump(str(best_score), json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    next_generation = []\n",
    "\n",
    "    next_generation = copy.deepcopy(generation)\n",
    "    for index, genetic in enumerate(next_generation):\n",
    "        if random.random() < Mutation_rate:\n",
    "            genetic.mutate()\n",
    "\n",
    "    next_generation.append(copy.deepcopy(best_optim))\n",
    "    next_generation.append(Momentum())\n",
    "    next_generation.append(RMSProp())\n",
    "    \n",
    "    generation = next_generation[len(next_generation) - Population:]\n",
    "\n",
    "    # pickleファイルが思ったより容量大きかったので、現世代の評価と進化が終わったら、ひとつ前の世代のpickleファイルを削除\n",
    "    check_path = folder_path + f\"/generation{progress -1}\"\n",
    "    pickle_list = glob.glob(check_path +\"/*.pickle\")\n",
    "    for path in pickle_list:\n",
    "        os.remove(path)\n",
    "\n",
    "    past_progress = progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf210_taro_clone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
